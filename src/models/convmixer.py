"""
ConvMixer Model Implementation - Full Model

Based on: "Patches Are All You Need?" by Trockman & Kolter (2022)

This file assembles the complete ConvMixer model:
1. Patch Embedding
2. Stack of ConvMixer Blocks
3. Global Average Pooling + Classifier

Date: 12/07/2025
Written by Zehao Li <zl3667@columbia.edu>

The code has not been generated by AI tools, or copied from an exteranal resource.
"""

import torch
import torch.nn as nn


class PatchEmbedding(nn.Module):
    """Patch Embedding Layer."""
    
    def __init__(self, in_channels=3, embed_dim=256, patch_size=7):
        super().__init__()
        self.proj = nn.Conv2d(in_channels, embed_dim, 
                              kernel_size=patch_size, stride=patch_size)
        self.act = nn.GELU()
        self.norm = nn.BatchNorm2d(embed_dim)
    
    def forward(self, x):
        x = self.proj(x)
        x = self.act(x)
        x = self.norm(x)
        return x


class ConvMixerBlock(nn.Module):
    """ConvMixer Block with depthwise and pointwise convolutions."""
    
    def __init__(self, dim, kernel_size=9):
        super().__init__()
        # Depthwise conv (spatial mixing)
        self.depthwise_conv = nn.Conv2d(dim, dim, kernel_size=kernel_size,
                                        groups=dim, padding='same')
        self.act1 = nn.GELU()
        self.bn1 = nn.BatchNorm2d(dim)
        
        # Pointwise conv (channel mixing)
        self.pointwise_conv = nn.Conv2d(dim, dim, kernel_size=1)
        self.act2 = nn.GELU()
        self.bn2 = nn.BatchNorm2d(dim)
    
    def forward(self, x):
        # Spatial mixing with residual
        residual = x
        x = self.depthwise_conv(x)
        x = self.act1(x)
        x = self.bn1(x)
        x = x + residual
        
        # Channel mixing
        x = self.pointwise_conv(x)
        x = self.act2(x)
        x = self.bn2(x)
        return x


class ConvMixer(nn.Module):
    """
    ConvMixer Model.
    
    A simple convolutional architecture that:
    1. Embeds input images as patches
    2. Applies repeated ConvMixer blocks for mixing
    3. Uses global pooling and linear classifier
    
    Naming convention: ConvMixer-{dim}/{depth}
    
    Args:
        dim: Hidden dimension (embedding dimension h)
        depth: Number of ConvMixer blocks (d)
        kernel_size: Kernel size for depthwise convolution (k)
        patch_size: Patch size for embedding (p)
        num_classes: Number of output classes
        in_channels: Number of input channels
    """
    
    def __init__(
        self,
        dim=256,
        depth=8,
        kernel_size=9,
        patch_size=7,
        num_classes=10,
        in_channels=3
    ):
        super().__init__()
        
        self.dim = dim
        self.depth = depth
        self.kernel_size = kernel_size
        self.patch_size = patch_size
        
        # Patch embedding
        self.patch_embed = PatchEmbedding(
            in_channels=in_channels,
            embed_dim=dim,
            patch_size=patch_size
        )
        
        # Stack of ConvMixer blocks
        self.blocks = nn.Sequential(*[
            ConvMixerBlock(dim=dim, kernel_size=kernel_size)
            for _ in range(depth)
        ])
        
        # Classification head
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.classifier = nn.Linear(dim, num_classes)
    
    def forward(self, x):
        """
        Forward pass.
        
        Args:
            x: Input tensor of shape (batch, channels, H, W)
        
        Returns:
            Class logits of shape (batch, num_classes)
        """
        # Patch embedding
        x = self.patch_embed(x)
        
        # Apply ConvMixer blocks
        x = self.blocks(x)
        
        # Classification
        x = self.pool(x)
        x = self.flatten(x)
        x = self.classifier(x)
        
        return x
    
    def get_internal_resolution(self, input_size):
        """Get the internal spatial resolution after patch embedding."""
        return input_size // self.patch_size


def convmixer_256_8(num_classes=10, **kwargs):
    """ConvMixer-256/8 for CIFAR-10 (paper configuration)."""
    return ConvMixer(dim=256, depth=8, kernel_size=9, patch_size=1,
                     num_classes=num_classes, **kwargs)


def convmixer_256_8_p2(num_classes=10, **kwargs):
    """ConvMixer-256/8 with patch_size=2."""
    return ConvMixer(dim=256, depth=8, kernel_size=9, patch_size=2,
                     num_classes=num_classes, **kwargs)


def convmixer_768_32(num_classes=1000, **kwargs):
    """ConvMixer-768/32 for ImageNet."""
    return ConvMixer(dim=768, depth=32, kernel_size=7, patch_size=7,
                     num_classes=num_classes, **kwargs)


def count_parameters(model):
    """Count the total number of trainable parameters."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def test_convmixer():
    """Test the full ConvMixer model."""
    print("Testing full ConvMixer model...")
    print("="*60)
    
    # Test CIFAR-10 configuration
    print("\n1. Testing ConvMixer-256/8 (CIFAR-10 config):")
    model = convmixer_256_8()
    x = torch.randn(2, 3, 32, 32)
    y = model(x)
    
    print(f"   Input shape: {x.shape}")
    print(f"   Output shape: {y.shape}")
    print(f"   Parameters: {count_parameters(model):,}")
    print(f"   Internal resolution: {model.get_internal_resolution(32)}x{model.get_internal_resolution(32)}")
    
    # Verify expected parameters (from paper Table 4)
    # ConvMixer-256/8 with k=9, p=1 should have ~707K params
    expected_approx = 700000
    actual = count_parameters(model)
    print(f"   Expected ~{expected_approx:,}, Actual: {actual:,}")
    
    # Test different kernel sizes
    print("\n2. Testing different kernel sizes:")
    for k in [3, 5, 7, 9]:
        model = ConvMixer(dim=256, depth=8, kernel_size=k, patch_size=1)
        params = count_parameters(model)
        print(f"   kernel_size={k}: {params:,} parameters")
    
    # Test different patch sizes
    print("\n3. Testing different patch sizes:")
    for p in [1, 2, 4]:
        model = ConvMixer(dim=256, depth=8, kernel_size=9, patch_size=p)
        internal_res = model.get_internal_resolution(32)
        params = count_parameters(model)
        print(f"   patch_size={p}: {params:,} params, internal res: {internal_res}x{internal_res}")
    
    print("\n" + "="*60)
    print("All ConvMixer tests passed!")


if __name__ == "__main__":
    test_convmixer()
