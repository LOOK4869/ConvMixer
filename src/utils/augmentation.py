"""
Data Augmentation for ConvMixer Training

Based on the paper, the following augmentations are used:
- RandAugment
- Mixup
- CutMix
- Random Erasing
- Standard augmentations (flip, crop)

Date: 12/11/2025
Written by [Minghao Liu] <ml5312@columbia.edu>

The code has not been generated by AI tools, or copied from an exteranal resource.
"""

import torch
import torchvision.transforms as transforms
import numpy as np


class Mixup:
    """
    Mixup augmentation.
    
    Creates convex combinations of pairs of examples and their labels.
    Reference: mixup: Beyond Empirical Risk Minimization (Zhang et al., 2017)
    """
    
    def __init__(self, alpha=0.8):
        self.alpha = alpha
    
    def __call__(self, images, labels):
        """
        Apply mixup to a batch.
        
        Args:
            images: Tensor of shape (batch, C, H, W)
            labels: Tensor of shape (batch,) - integer labels
            
        Returns:
            mixed_images, labels_a, labels_b, lam
        """
        if self.alpha > 0:
            lam = np.random.beta(self.alpha, self.alpha)
        else:
            lam = 1
        
        batch_size = images.size(0)
        index = torch.randperm(batch_size)
        
        mixed_images = lam * images + (1 - lam) * images[index]
        labels_a = labels
        labels_b = labels[index]
        
        return mixed_images, labels_a, labels_b, lam


class CutMix:
    """
    CutMix augmentation.
    
    Cuts and pastes patches among training images.
    Reference: CutMix: Regularization Strategy (Yun et al., 2019)
    """
    
    def __init__(self, alpha=1.0):
        self.alpha = alpha
    
    def __call__(self, images, labels):
        """
        Apply cutmix to a batch.
        
        Args:
            images: Tensor of shape (batch, C, H, W)
            labels: Tensor of shape (batch,)
            
        Returns:
            mixed_images, labels_a, labels_b, lam
        """
        if self.alpha > 0:
            lam = np.random.beta(self.alpha, self.alpha)
        else:
            lam = 1
        
        batch_size = images.size(0)
        index = torch.randperm(batch_size)
        
        # Get bounding box
        W = images.size(2)
        H = images.size(3)
        
        cut_ratio = np.sqrt(1 - lam)
        cut_w = int(W * cut_ratio)
        cut_h = int(H * cut_ratio)
        
        cx = np.random.randint(W)
        cy = np.random.randint(H)
        
        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)
        
        # Apply cutmix
        mixed_images = images.clone()
        mixed_images[:, :, bbx1:bbx2, bby1:bby2] = images[index, :, bbx1:bbx2, bby1:bby2]
        
        # Adjust lambda based on actual area
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
        
        return mixed_images, labels, labels[index], lam


def mixup_criterion(criterion, pred, labels_a, labels_b, lam):
    """Compute mixup loss."""
    return lam * criterion(pred, labels_a) + (1 - lam) * criterion(pred, labels_b)


def get_train_transforms(use_randaugment=True, randaugment_n=2, randaugment_m=9):
    """
    Get training transforms for CIFAR-10.
    
    Args:
        use_randaugment: Whether to use RandAugment
        randaugment_n: Number of augmentation operations
        randaugment_m: Magnitude of augmentation
    """
    transforms_list = [
        transforms.RandomHorizontalFlip(),
        transforms.RandomCrop(32, padding=4),
    ]
    
    if use_randaugment:
        transforms_list.append(
            transforms.RandAugment(num_ops=randaugment_n, magnitude=randaugment_m)
        )
    
    transforms_list.extend([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=0.25),
    ])
    
    return transforms.Compose(transforms_list)


def get_test_transforms():
    """Get test/validation transforms."""
    return transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])


def visualize_augmentations(images, save_path=None):
    """
    Visualize a batch of augmented images.
    
    Args:
        images: Tensor of shape (batch, C, H, W)
        save_path: Optional path to save the figure
    """
    import matplotlib.pyplot as plt
    
    # Denormalize
    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)
    std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)
    images = images * std + mean
    images = torch.clamp(images, 0, 1)
    
    # Plot
    n = min(8, images.size(0))
    fig, axes = plt.subplots(2, n//2, figsize=(12, 6))
    
    for i, ax in enumerate(axes.flat):
        if i < n:
            ax.imshow(images[i].permute(1, 2, 0).numpy())
            ax.axis('off')
    
    plt.suptitle('Augmented Images')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150)
    plt.show()


if __name__ == "__main__":
    # Test augmentations
    print("Testing augmentations...")
    
    # Test Mixup
    mixup = Mixup(alpha=0.8)
    images = torch.randn(4, 3, 32, 32)
    labels = torch.tensor([0, 1, 2, 3])
    mixed, la, lb, lam = mixup(images, labels)
    print(f"Mixup - lambda: {lam:.3f}")
    
    # Test CutMix
    cutmix = CutMix(alpha=1.0)
    mixed, la, lb, lam = cutmix(images, labels)
    print(f"CutMix - lambda: {lam:.3f}")
    
    # Test transforms
    train_transform = get_train_transforms()
    test_transform = get_test_transforms()
    print(f"\nTrain transforms: {train_transform}")
    print(f"\nTest transforms: {test_transform}")
    
    print("\nAugmentation tests passed!")
