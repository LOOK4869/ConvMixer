"""
Complete Training Script for ConvMixer

This script implements the full training procedure from the paper:
- AdamW optimizer
- Triangular/Cosine learning rate schedule
- Gradient norm clipping
- Mixup/CutMix augmentation
- Logging and checkpointing

Date: 12/12/2025
Written by [Minghao Liu] <ml5312@columbia.edu>

The code has not been generated by AI tools, or copied from an exteranal resource.
"""

import os
import json
import time
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import matplotlib.pyplot as plt

# Import our modules
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.models.convmixer import ConvMixer, count_parameters
from src.utils.augmentation import Mixup, CutMix, mixup_criterion
from src.utils.augmentation import get_train_transforms, get_test_transforms


class TriangularLR:
    """
    Triangular learning rate schedule.
    Increases linearly for first half, decreases for second half.
    """
    def __init__(self, optimizer, max_lr, total_steps):
        self.optimizer = optimizer
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.current_step = 0
    
    def step(self):
        self.current_step += 1
        half = self.total_steps // 2
        
        if self.current_step <= half:
            lr = self.max_lr * (self.current_step / half)
        else:
            lr = self.max_lr * (2 - self.current_step / half)
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = max(lr, 1e-6)
    
    def get_lr(self):
        return self.optimizer.param_groups[0]['lr']


class CosineLR:
    """Cosine annealing with warmup."""
    def __init__(self, optimizer, max_lr, total_steps, warmup_steps=500):
        self.optimizer = optimizer
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.warmup_steps = warmup_steps
        self.current_step = 0
    
    def step(self):
        self.current_step += 1
        
        if self.current_step <= self.warmup_steps:
            lr = self.max_lr * (self.current_step / self.warmup_steps)
        else:
            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr = self.max_lr * 0.5 * (1 + np.cos(np.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = max(lr, 1e-6)
    
    def get_lr(self):
        return self.optimizer.param_groups[0]['lr']


class Trainer:
    """
    Trainer class for ConvMixer.
    """
    
    def __init__(
        self,
        model,
        trainloader,
        testloader,
        config,
        device='cuda'
    ):
        self.model = model.to(device)
        self.trainloader = trainloader
        self.testloader = testloader
        self.config = config
        self.device = device
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss()
        
        # Optimizer
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config['lr'],
            weight_decay=config['weight_decay']
        )
        
        # Learning rate scheduler
        total_steps = config['epochs'] * len(trainloader)
        if config['lr_schedule'] == 'triangular':
            self.scheduler = TriangularLR(self.optimizer, config['lr'], total_steps)
        else:
            self.scheduler = CosineLR(self.optimizer, config['lr'], total_steps)
        
        # Augmentation
        self.mixup = Mixup(alpha=config.get('mixup_alpha', 0.8)) if config.get('use_mixup', True) else None
        self.cutmix = CutMix(alpha=config.get('cutmix_alpha', 1.0)) if config.get('use_cutmix', True) else None
        
        # History
        self.history = {
            'train_loss': [], 'train_acc': [],
            'test_loss': [], 'test_acc': [],
            'lr': []
        }
        
        # Best accuracy
        self.best_acc = 0
        
        # Log directory
        self.log_dir = config.get('log_dir', './logs')
        os.makedirs(self.log_dir, exist_ok=True)
    
    def train_epoch(self):
        """Train for one epoch."""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(self.trainloader, desc='Training')
        for images, labels in pbar:
            images, labels = images.to(self.device), labels.to(self.device)
            
            # Apply mixup or cutmix with 50% probability each
            use_mixing = False
            if self.mixup and self.cutmix:
                if np.random.rand() < 0.5:
                    images, labels_a, labels_b, lam = self.mixup(images, labels)
                    use_mixing = True
                elif np.random.rand() < 0.5:
                    images, labels_a, labels_b, lam = self.cutmix(images, labels)
                    use_mixing = True
            
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(images)
            
            # Compute loss
            if use_mixing:
                loss = mixup_criterion(self.criterion, outputs, labels_a, labels_b, lam)
            else:
                loss = self.criterion(outputs, labels)
            
            # Backward pass with gradient clipping
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['clip_grad'])
            self.optimizer.step()
            self.scheduler.step()
            
            # Statistics
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            if not use_mixing:
                correct += predicted.eq(labels).sum().item()
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{self.scheduler.get_lr():.6f}'})
        
        epoch_loss = total_loss / len(self.trainloader)
        epoch_acc = 100. * correct / total if correct > 0 else 0
        
        return epoch_loss, epoch_acc
    
    def evaluate(self):
        """Evaluate on test set."""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in self.testloader:
                images, labels = images.to(self.device), labels.to(self.device)
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return total_loss / len(self.testloader), 100. * correct / total
    
    def train(self):
        """Full training loop."""
        print(f"\nTraining ConvMixer on {self.device}")
        print(f"Parameters: {count_parameters(self.model):,}")
        print(f"Epochs: {self.config['epochs']}")
        print("="*60)
        
        start_time = time.time()
        
        for epoch in range(self.config['epochs']):
            # Train
            train_loss, train_acc = self.train_epoch()
            
            # Evaluate
            test_loss, test_acc = self.evaluate()
            
            # Record history
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['test_loss'].append(test_loss)
            self.history['test_acc'].append(test_acc)
            self.history['lr'].append(self.scheduler.get_lr())
            
            # Print progress
            print(f"\nEpoch {epoch+1}/{self.config['epochs']}")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")
            
            # Save best model
            if test_acc > self.best_acc:
                self.best_acc = test_acc
                self.save_checkpoint('best_model.pth')
                print(f"  New best accuracy: {test_acc:.2f}%")
        
        total_time = time.time() - start_time
        print(f"\nTraining completed in {total_time/60:.2f} minutes")
        print(f"Best Test Accuracy: {self.best_acc:.2f}%")
        
        # Save final results
        self.save_results()
        self.plot_curves()
        
        return self.history
    
    def save_checkpoint(self, filename):
        """Save model checkpoint."""
        path = os.path.join(self.log_dir, filename)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_acc': self.best_acc,
            'config': self.config
        }, path)
    
    def save_results(self):
        """Save training results."""
        results = {
            'config': self.config,
            'history': self.history,
            'best_acc': self.best_acc,
            'num_params': count_parameters(self.model)
        }
        path = os.path.join(self.log_dir, 'results.json')
        with open(path, 'w') as f:
            json.dump(results, f, indent=2)
    
    def plot_curves(self):
        """Plot and save training curves."""
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        epochs = range(1, len(self.history['train_loss']) + 1)
        
        # Loss
        axes[0].plot(epochs, self.history['train_loss'], 'b-', label='Train')
        axes[0].plot(epochs, self.history['test_loss'], 'r-', label='Test')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('Loss Curves')
        axes[0].legend()
        axes[0].grid(True)
        
        # Accuracy
        axes[1].plot(epochs, self.history['test_acc'], 'g-')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Accuracy (%)')
        axes[1].set_title(f'Test Accuracy (Best: {self.best_acc:.2f}%)')
        axes[1].grid(True)
        
        # Learning rate
        axes[2].plot(epochs, self.history['lr'], 'orange')
        axes[2].set_xlabel('Epoch')
        axes[2].set_ylabel('Learning Rate')
        axes[2].set_title('Learning Rate Schedule')
        axes[2].grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.log_dir, 'training_curves.png'), dpi=150)
        plt.close()


def main():
    parser = argparse.ArgumentParser(description='Train ConvMixer on CIFAR-10')
    parser.add_argument('--dim', type=int, default=256, help='Hidden dimension')
    parser.add_argument('--depth', type=int, default=8, help='Number of blocks')
    parser.add_argument('--kernel_size', type=int, default=9, help='Kernel size')
    parser.add_argument('--patch_size', type=int, default=1, help='Patch size')
    parser.add_argument('--epochs', type=int, default=200, help='Number of epochs')
    parser.add_argument('--batch_size', type=int, default=64, help='Batch size')
    parser.add_argument('--lr', type=float, default=0.01, help='Learning rate')
    parser.add_argument('--log_dir', type=str, default='./logs', help='Log directory')
    args = parser.parse_args()
    
    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Data
    train_transform = get_train_transforms()
    test_transform = get_test_transforms()
    
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=train_transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                           download=True, transform=test_transform)
    
    trainloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)
    
    # Model
    model = ConvMixer(
        dim=args.dim,
        depth=args.depth,
        kernel_size=args.kernel_size,
        patch_size=args.patch_size,
        num_classes=10
    )
    
    # Config
    config = {
        'dim': args.dim,
        'depth': args.depth,
        'kernel_size': args.kernel_size,
        'patch_size': args.patch_size,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'lr': args.lr,
        'weight_decay': 0.01,
        'clip_grad': 1.0,
        'lr_schedule': 'triangular',
        'use_mixup': True,
        'use_cutmix': True,
        'log_dir': args.log_dir
    }
    
    # Train
    trainer = Trainer(model, trainloader, testloader, config, device)
    trainer.train()


if __name__ == "__main__":
    main()
